{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running PyTest in Jupyter Notebooks\n",
    "\n",
    "> iPyMock uses GPT 3.5 turbo by browser automation and [a CoSENT based model](https://huggingface.co/GanymedeNil/text2vec-large-chinese) to embed English and Chinese.\n",
    ">\n",
    "> ---\n",
    "This project for interactive PyTest is created by the [NbDev Template](https://youtu.be/ZJTop5uqC2U)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Discord Follow](https://dcbadge.vercel.app/api/server/8YhXA7TYrC?style=flat)](https://discord.gg/8YhXA7TYrC)\n",
    "[![Twitter Follow](https://img.shields.io/twitter/follow/seii_saintway?style=social)](https://twitter.com/seii_saintway)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup iPyMock\n",
    "\n",
    "`gmail_address` and `gmail_password` are needed for utilizing chrome automation locally.\n",
    "\n",
    "`conversation_id` could be found in the url of `chat.openai.com/c/<conversation_id>`.\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.config/ipymock\n",
    "\n",
    "cat << EOF > ~/.config/ipymock/config.json\n",
    "{\n",
    "  \"email\": \"<gmail_address>\",\n",
    "  \"password\": \"<gmail_password>\",\n",
    "  \"conversation_id\": \"<conversation_id>\"\n",
    "}\n",
    "EOF\n",
    "\n",
    "pip install --upgrade ipymock\n",
    "```\n",
    "\n",
    "`access_token` at [openai api session](https://chat.openai.com/api/auth/session) is needed for utilizing a backend api proxy.\n",
    "\n",
    "```bash\n",
    "mkdir -p ~/.config/ipymock\n",
    "\n",
    "cat << EOF > ~/.config/ipymock/config.json\n",
    "{\n",
    "  \"chat_gpt_base_url\": \"<chat_gpt_base_url>\",\n",
    "  \"access_token\": \"<access_token>\",\n",
    "  \"conversation_id\": \"<conversation_id>\"\n",
    "}\n",
    "EOF\n",
    "\n",
    "pip install --upgrade ipymock\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OpenAI Backend API from Browser Side in Jupyter Notebooks\n",
    "\n",
    "```python\n",
    "from ipymock.browser import start_conversation\n",
    "import IPython\n",
    "\n",
    "def ask(prompt):\n",
    "    for response in start_conversation(prompt):\n",
    "        IPython.display.display(IPython.core.display.Markdown(response))\n",
    "        IPython.display.clear_output(wait=True)\n",
    "\n",
    "import ipymock.browser\n",
    "# 1. you could initialize chrome automation locally\n",
    "ipymock.browser.init(['--headless'])\n",
    "# 2. or if a proxy is deployed locally\n",
    "ipymock.browser.common.chat_gpt_base_url = 'http://127.0.0.1:8080'\n",
    "# 3. otherwise using a third party proxy\n",
    "ipymock.browser.common.chat_gpt_base_url = 'https://.../api'\n",
    "# the conversation_id which is set in config.json\n",
    "print(ipymock.browser.common.conversation_id)\n",
    "\n",
    "ask('''\n",
    "what is the meaning of getting patched?\n",
    "''')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing AutoGPT\n",
    "\n",
    "This is the test function for AutoGPT.\n",
    "\n",
    "```python\n",
    "import os, sys\n",
    "os.chdir(os.path.expanduser('~/Auto-GPT'))\n",
    "sys.path.append(os.path.expanduser('~/Auto-GPT'))\n",
    "\n",
    "def test_auto_gpt(\n",
    "    mock_openai,\n",
    "    mock_openai_embed,\n",
    "    reset_embed_dimension,\n",
    "):\n",
    "    from autogpt.main import run_auto_gpt\n",
    "    run_auto_gpt(\n",
    "        continuous = True,\n",
    "        continuous_limit = 10000,\n",
    "        ai_settings = None,\n",
    "        skip_reprompt = False,\n",
    "        speak = True,\n",
    "        debug = False,\n",
    "        gpt3only = False,\n",
    "        gpt4only = True,\n",
    "        memory_type = 'local',\n",
    "        browser_name = 'safari',\n",
    "        allow_downloads = True,\n",
    "        skip_news = True,\n",
    "        workspace_directory = os.path.expanduser('~/Auto-GPT/andrew_space'),\n",
    "        install_plugin_deps = True,\n",
    "    )\n",
    "    assert True\n",
    "```\n",
    "\n",
    "It actually run AutoGPT by pytest mock and browser automation.\n",
    "\n",
    "```python\n",
    "import ipymock\n",
    "import ipymock.browser\n",
    "import ipymock.llm\n",
    "import pytest\n",
    "\n",
    "# reset conversation_id to empty to start a new chat\n",
    "ipymock.browser.common.conversation_id = ''\n",
    "ipymock.browser.init()\n",
    "\n",
    "@pytest.fixture\n",
    "def reset_embed_dimension(monkeypatch):\n",
    "    import autogpt.memory.local\n",
    "    monkeypatch.setattr(autogpt.memory.local, 'EMBED_DIM', 1024)\n",
    "\n",
    "ipymock.do(\n",
    "    mock_openai = ipymock.browser.mock_openai,\n",
    "    mock_openai_embed = ipymock.llm.mock_openai_embed,\n",
    "    reset_embed_dimension = reset_embed_dimension,\n",
    "    test_auto_gpt = test_auto_gpt,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project is still under development and lack of documentation.\n",
    "\n",
    "This is [an article](https://seii-saintway.github.io/2023/04/06/Autonomous-Agents/) I wrote that mock openai to test autonomous robots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Mechanism of PyTesting the Python Testfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pluggy\n",
    "pm = pluggy.PluginManager('pytest')\n",
    "\n",
    "import _pytest.hookspec\n",
    "pm.add_hookspecs(_pytest.hookspec)\n",
    "\n",
    "import _pytest.main\n",
    "pm.register(_pytest.main, 'main')\n",
    "\n",
    "import _pytest.config\n",
    "cfg = _pytest.config.get_config()\n",
    "cfg.parse(args=[])\n",
    "pm.hook.pytest_cmdline_main(config=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pytest.config\n",
    "cfg = _pytest.config.get_config()\n",
    "cfg.parse(args=[])\n",
    "cfg.pluginmanager.hook.pytest_cmdline_main(config=cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTesting the Python Testcases within iPyNb Runtimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content of test_time.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "testdata = [\n",
    "    (datetime(2001, 12, 12), datetime(2001, 12, 11), timedelta(1)),\n",
    "    (datetime(2001, 12, 11), datetime(2001, 12, 12), timedelta(-1)),\n",
    "]\n",
    "\n",
    "\n",
    "def idfn(val):\n",
    "    if isinstance(val, (datetime,)):\n",
    "        # note this wouldn't show any hours/minutes/seconds\n",
    "        return val.strftime('%Y%m%d')\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize('a,b,expected', testdata)\n",
    "def test_timedistance_v0(a, b, expected):\n",
    "    diff = a - b\n",
    "    assert diff == expected\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize('a,b,expected', testdata, ids=['forward', 'backward'])\n",
    "def test_timedistance_v1(a, b, expected):\n",
    "    diff = a - b\n",
    "    assert diff == expected\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize('a,b,expected', testdata, ids=idfn)\n",
    "def test_timedistance_v2(a, b, expected):\n",
    "    diff = a - b\n",
    "    assert diff == expected\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    'a,b,expected',\n",
    "    [\n",
    "        pytest.param(\n",
    "            datetime(2001, 12, 12), datetime(2001, 12, 11), timedelta(1), id='forward'\n",
    "        ),\n",
    "        pytest.param(\n",
    "            datetime(2001, 12, 11), datetime(2001, 12, 12), timedelta(-1), id='backward'\n",
    "        ),\n",
    "    ],\n",
    ")\n",
    "def test_timedistance_v3(a, b, expected):\n",
    "    diff = a - b\n",
    "    assert diff != expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import _pytest.config\n",
    "cfg = _pytest.config.get_config()\n",
    "cfg.parse(args=[])\n",
    "\n",
    "import _pytest.main\n",
    "ss = _pytest.main.Session.from_config(cfg)\n",
    "import _pytest.runner\n",
    "ss._setupstate = _pytest.runner.SetupState()\n",
    "import _pytest.fixtures\n",
    "ss._fixturemanager = _pytest.fixtures.FixtureManager(ss)\n",
    "\n",
    "import _pytest.python\n",
    "import py\n",
    "m = _pytest.python.Module.from_parent(parent=ss, fspath=py.path.local())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Object(object):\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "m.obj = Object(**globals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx = 0\n",
      "<TestReport '::unittests::test_timedistance_v0[a0-b0-expected0]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v0[a0-b0-expected0]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v0[a0-b0-expected0]' when='teardown' outcome='passed'>\n",
      "idx = 1\n",
      "<TestReport '::unittests::test_timedistance_v0[a1-b1-expected1]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v0[a1-b1-expected1]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v0[a1-b1-expected1]' when='teardown' outcome='passed'>\n",
      "idx = 2\n",
      "<TestReport '::unittests::test_timedistance_v1[forward]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v1[forward]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v1[forward]' when='teardown' outcome='passed'>\n",
      "idx = 3\n",
      "<TestReport '::unittests::test_timedistance_v1[backward]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v1[backward]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v1[backward]' when='teardown' outcome='passed'>\n",
      "idx = 4\n",
      "<TestReport '::unittests::test_timedistance_v2[20011212-20011211-expected0]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v2[20011212-20011211-expected0]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v2[20011212-20011211-expected0]' when='teardown' outcome='passed'>\n",
      "idx = 5\n",
      "<TestReport '::unittests::test_timedistance_v2[20011211-20011212-expected1]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v2[20011211-20011212-expected1]' when='call' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v2[20011211-20011212-expected1]' when='teardown' outcome='passed'>\n",
      "idx = 6\n",
      "<TestReport '::unittests::test_timedistance_v3[forward]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v3[forward]' when='call' outcome='failed'>\n",
      "<TestReport '::unittests::test_timedistance_v3[forward]' when='teardown' outcome='passed'>\n",
      "idx = 7\n",
      "<TestReport '::unittests::test_timedistance_v3[backward]' when='setup' outcome='passed'>\n",
      "<TestReport '::unittests::test_timedistance_v3[backward]' when='call' outcome='failed'>\n",
      "<TestReport '::unittests::test_timedistance_v3[backward]' when='teardown' outcome='passed'>\n"
     ]
    }
   ],
   "source": [
    "import _pytest.runner\n",
    "c = dict(enumerate(m.collect()))\n",
    "for i in c:\n",
    "    print(f'idx = {i}')\n",
    "    print(_pytest.runner.call_and_report(c[i], 'setup'))\n",
    "    print(_pytest.runner.call_and_report(c[i], 'call'))\n",
    "    print(_pytest.runner.call_and_report(c[i], 'teardown', nextitem=c.get(i+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "for i, f in enumerate(m.collect()):\n",
    "    print(f'idx = {i}')\n",
    "    f.setup()\n",
    "    f.runtest()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Use the Do-PyTest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "import os\n",
    "nbs_dir = os.getcwd()\n",
    "project_dir = os.path.dirname(nbs_dir)\n",
    "\n",
    "import sys\n",
    "sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture_1(tmpdir_factory):\n",
    "    return tmpdir_factory\n",
    "\n",
    "@pytest.fixture\n",
    "def my_fixture_2(tmpdir_factory):\n",
    "    return tmpdir_factory\n",
    "\n",
    "def test_fixture(my_fixture_1, my_fixture_2):\n",
    "    assert my_fixture_1 == my_fixture_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipymock import do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=> no.0  nbs::test_fixture  setup  passed\n",
      "\n",
      "=> no.0  nbs::test_fixture  runtest  passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "do(\n",
    "    my_fixture_1=my_fixture_1,\n",
    "    my_fixture_2=my_fixture_2,\n",
    "    test_fixture=test_fixture\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Tips of NbDev\n",
    "\n",
    "-  Make sure you are using the latest version of nbdev with `pip install --upgrade nbdev`\n",
    "-  If you are using an older version of this template, see the instructions above on how to upgrade your template. \n",
    "-  It is important for you to spell the name of your user and repo correctly in `settings.ini` or the website will not have the correct address from which to source assets like CSS for your site.  When in doubt, you can open your browser's developer console and see if there are any errors related to fetching assets for your website due to an incorrect URL generated by misspelled values from `settings.ini`.\n",
    "-  If you change the name of your repo, you have to make the appropriate changes in `settings.ini`\n",
    "-  After you make changes to `settings.ini`, run `nbdev_build_lib && nbdev_clean_nbs && nbdev_build_docs` to make sure all changes are propagated appropriately."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
